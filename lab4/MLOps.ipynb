{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "edb4d618",
   "metadata": {},
   "source": [
    "### ЛАБОРАТОРНА РОБОТА 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd8fed7",
   "metadata": {},
   "source": [
    "#### 1. Вибір задачі та датасету"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6102bc3b",
   "metadata": {},
   "source": [
    "Використаємо датасет та модель з лабораторної №2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3bd5d330",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yaroslavp/workspace/master_IASA/nlp/lab4/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'T5Tokenizer'. \n",
      "The class this function is called from is 'MT5Tokenizer'.\n",
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import MT5Tokenizer, MT5ForConditionalGeneration, pipeline\n",
    "\n",
    "model_name = \"../lab2/models/mt5_fr2en_copy/checkpoint-500\"\n",
    "tokenizer = MT5Tokenizer.from_pretrained(model_name)\n",
    "model = MT5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "generator = pipeline(\"text2text-generation\", model=model.eval(), tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd33bae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=256) and `max_length`(=128) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Hello, how are you doing?'}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'Bonjour comment allez-vous?'\n",
    "\n",
    "outputs = generator(\n",
    "    \"translate French to English: \" + text,\n",
    "    max_length=128,\n",
    "    do_sample=True,\n",
    "    temperature=0.4\n",
    ")\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f448b64f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello, how are you doing?'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[0]['generated_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1cb2ec0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'datasets'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgptqmodel\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GPTQModel, QuantizeConfig\n\u001b[32m      4\u001b[39m model_id = model_name\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'datasets'"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from gptqmodel import GPTQModel, QuantizeConfig\n",
    "\n",
    "model_id = model_name\n",
    "quant_path = \"api/service/model/mt5_fr2en-gptqmodel-4bit\"\n",
    "\n",
    "calibration_dataset = load_dataset(\n",
    "    \"allenai/c4\",\n",
    "    data_files=\"en/c4-train.00001-of-01024.json.gz\",\n",
    "    split=\"train\"\n",
    "  ).select(range(1024))[\"text\"]\n",
    "\n",
    "quant_config = QuantizeConfig(bits=4, group_size=128)\n",
    "\n",
    "model = GPTQModel.load(model_id, quant_config)\n",
    "\n",
    "# increase `batch_size` to match gpu/vram specs to speed up quantization\n",
    "model.quantize(calibration_dataset, batch_size=1)\n",
    "\n",
    "model.save(quant_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4fb8bb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.6\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.version.cuda)         # e.g., '11.8'\n",
    "print(torch.cuda.is_available())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
