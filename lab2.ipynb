{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "304bdaee",
   "metadata": {},
   "source": [
    "### ЛАБОРАТОРНА РОБОТА 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9139df",
   "metadata": {},
   "source": [
    "#### 1. Вибір задачі та датасету"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063b7738",
   "metadata": {},
   "source": [
    "https://huggingface.co/datasets/Helsinki-NLP/opus-100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63fedb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "language_pairs = [\"en-fr\"] # \"en-fi\", \"ar-en\", \"en-hi\", \"en-zh\"\n",
    "subset_size = 1000  # number of training samples per pair\n",
    "\n",
    "saveDatasetTo = 'datasets/opus-100/train.json'\n",
    "\n",
    "if os.path.exists(saveDatasetTo):\n",
    "    data = pd.read_json(saveDatasetTo)\n",
    "else:\n",
    "    data = pd.DataFrame()\n",
    "\n",
    "    for pair in language_pairs:\n",
    "        print(f\"Loading {pair}...\")\n",
    "        data[pair]= load_dataset(\"Helsinki-NLP/opus-100\", pair, split=f\"train[:{subset_size}]\")\n",
    "    \n",
    "    data.to_json(saveDatasetTo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f777bd9",
   "metadata": {},
   "source": [
    "#### 2. Аналіз даних та метрик"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3b8cc80a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>en-fr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>{'translation': {'en': 'Thank you.', 'fr': 'Me...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    en-fr\n",
       "count                                                1000\n",
       "unique                                                985\n",
       "top     {'translation': {'en': 'Thank you.', 'fr': 'Me...\n",
       "freq                                                    6"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "caf3a3ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1000 entries, 0 to 999\n",
      "Data columns (total 1 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   en-fr   1000 non-null   object\n",
      "dtypes: object(1)\n",
      "memory usage: 15.6+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "53f571e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "en-fr    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0b6e772",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['en', 'fr'],\n",
       "    num_rows: 1000\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(data[\"en-fr\"].tolist())\n",
    "df = df[\"translation\"].apply(pd.Series)\n",
    "data = Dataset.from_pandas(df)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e74a9d01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'en': 'Why at my place?', 'fr': 'Pourquoi chez moi ?'}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[18]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02124900",
   "metadata": {},
   "source": [
    "Метрики для оцінки моделей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4a356a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EvaluationModule(name: \"bleu\", module_type: \"metric\", features: [{'predictions': Value(dtype='string', id='sequence'), 'references': Sequence(feature=Value(dtype='string', id='sequence'), length=-1, id='references')}, {'predictions': Value(dtype='string', id='sequence'), 'references': Value(dtype='string', id='sequence')}], usage: \"\"\"\n",
      "Computes BLEU score of translated segments against one or more references.\n",
      "Args:\n",
      "    predictions: list of translations to score.\n",
      "    references: list of lists of or just a list of references for each translation.\n",
      "    tokenizer : approach used for tokenizing `predictions` and `references`.\n",
      "        The default tokenizer is `tokenizer_13a`, a minimal tokenization approach that is equivalent to `mteval-v13a`, used by WMT.\n",
      "        This can be replaced by any function that takes a string as input and returns a list of tokens as output.\n",
      "    max_order: Maximum n-gram order to use when computing BLEU score.\n",
      "    smooth: Whether or not to apply Lin et al. 2004 smoothing.\n",
      "Returns:\n",
      "    'bleu': bleu score,\n",
      "    'precisions': geometric mean of n-gram precisions,\n",
      "    'brevity_penalty': brevity penalty,\n",
      "    'length_ratio': ratio of lengths,\n",
      "    'translation_length': translation_length,\n",
      "    'reference_length': reference_length\n",
      "Examples:\n",
      "\n",
      "    >>> predictions = [\"hello there general kenobi\", \"foo bar foobar\"]\n",
      "    >>> references = [\n",
      "    ...     [\"hello there general kenobi\", \"hello there!\"],\n",
      "    ...     [\"foo bar foobar\"]\n",
      "    ... ]\n",
      "    >>> bleu = evaluate.load(\"bleu\")\n",
      "    >>> results = bleu.compute(predictions=predictions, references=references)\n",
      "    >>> print(results[\"bleu\"])\n",
      "    1.0\n",
      "\"\"\", stored examples: 0)\n",
      "EvaluationModule(name: \"bert_score\", module_type: \"metric\", features: [{'predictions': Value(dtype='string', id='sequence'), 'references': Sequence(feature=Value(dtype='string', id='sequence'), length=-1, id='references')}, {'predictions': Value(dtype='string', id='sequence'), 'references': Value(dtype='string', id='sequence')}], usage: \"\"\"\n",
      "BERTScore Metrics with the hashcode from a source against one or more references.\n",
      "\n",
      "Args:\n",
      "    predictions (list of str): Prediction/candidate sentences.\n",
      "    references (list of str or list of list of str): Reference sentences.\n",
      "    lang (str): Language of the sentences; required (e.g. 'en').\n",
      "    model_type (str): Bert specification, default using the suggested\n",
      "        model for the target language; has to specify at least one of\n",
      "        `model_type` or `lang`.\n",
      "    num_layers (int): The layer of representation to use,\n",
      "        default using the number of layers tuned on WMT16 correlation data.\n",
      "    verbose (bool): Turn on intermediate status update.\n",
      "    idf (bool or dict): Use idf weighting; can also be a precomputed idf_dict.\n",
      "    device (str): On which the contextual embedding model will be allocated on.\n",
      "        If this argument is None, the model lives on cuda:0 if cuda is available.\n",
      "    nthreads (int): Number of threads.\n",
      "    batch_size (int): Bert score processing batch size,\n",
      "        at least one of `model_type` or `lang`. `lang` needs to be\n",
      "        specified when `rescale_with_baseline` is True.\n",
      "    rescale_with_baseline (bool): Rescale bertscore with pre-computed baseline.\n",
      "    baseline_path (str): Customized baseline file.\n",
      "    use_fast_tokenizer (bool): `use_fast` parameter passed to HF tokenizer. New in version 0.3.10.\n",
      "\n",
      "Returns:\n",
      "    precision: Precision.\n",
      "    recall: Recall.\n",
      "    f1: F1 score.\n",
      "    hashcode: Hashcode of the library.\n",
      "\n",
      "Examples:\n",
      "\n",
      "    >>> predictions = [\"hello there\", \"general kenobi\"]\n",
      "    >>> references = [\"hello there\", \"general kenobi\"]\n",
      "    >>> bertscore = evaluate.load(\"bertscore\")\n",
      "    >>> results = bertscore.compute(predictions=predictions, references=references, lang=\"en\")\n",
      "    >>> print([round(v, 2) for v in results[\"f1\"]])\n",
      "    [1.0, 1.0]\n",
      "\"\"\", stored examples: 0)\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "bleu_score = evaluate.load(\"bleu\")\n",
    "bert_score = evaluate.load(\"bertscore\")\n",
    "\n",
    "def evaluate_metrics(decoded_preds, decoded_labels):\n",
    "    result_bleu = bleu_score.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    result_bertscore = bert_score.compute(predictions=decoded_preds, references=decoded_labels, lang=\"en\")\n",
    "\n",
    "    result = {\n",
    "        \"bleu\": result_bleu[\"bleu\"], \n",
    "        \"bertscore_f1\": np.mean(result_bertscore[\"f1\"]),\n",
    "        \"bertscore_precision\": np.mean(result_bertscore[\"precision\"]),\n",
    "        \"bertscore_recall\": np.mean(result_bertscore[\"recall\"])\n",
    "    }\n",
    "\n",
    "    result = {k: round(v, 4) for k, v in result.items()}\n",
    "    return result\n",
    "\n",
    "print(bleu_score)\n",
    "print(bert_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829b7e7c",
   "metadata": {},
   "source": [
    "Бенчмарки"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aeb8ebd",
   "metadata": {},
   "source": [
    "#### 3. Експериментальна частина\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767997c8",
   "metadata": {},
   "source": [
    "Бейслайн: **grok3**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe162b4",
   "metadata": {},
   "source": [
    "Варто зазначити деякі недоліки в результатах від grok3:\n",
    "- в перекладі був пропущений 1 рядок (елемент), тому довелось власноруч його заповнити аби усі результати не з'їхали на один індекс\n",
    "- деякі речення розбились на декілька колонок, тож їх потрібно було об'єднати"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8366bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label: Open the gate!\n",
      "pred: Open the gate quickly!\n",
      "\n",
      "label: No! Tara?\n",
      "pred: Tara?\n",
      "\n",
      "label: Who do you work for?\n",
      "pred: Who do you work for?\n",
      "\n",
      "label: Where have you been?\n",
      "pred: Where were you then?\n",
      "\n",
      "label: \"Tetsutaro.\n",
      "pred: Tetsutaro.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yaroslavp/workspace/master_IASA/iasa_nlp_course/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'bleu': 0.3667,\n",
       " 'bertscore_f1': 0.9395,\n",
       " 'bertscore_precision': 0.9344,\n",
       " 'bertscore_recall': 0.945}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = pd.read_csv('datasets/opus-100/en-fr_labels.csv')\n",
    "predicted = pd.read_csv('datasets/opus-100/grok3_en-fr_preds.csv')\n",
    "\n",
    "labels = labels['en-fr']\n",
    "predicted = predicted['en-fr']\n",
    "\n",
    "\n",
    "for label, pred in list(zip(labels, predicted))[:5]:\n",
    "    print('label:', label)\n",
    "    print('pred:', pred)\n",
    "    print()\n",
    "\n",
    "evaluate_metrics(labels, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a2a92e",
   "metadata": {},
   "source": [
    "### Файнтюн encoder-decoder моделі"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34a09818",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def set_seeds(seed):\n",
    "    \"\"\"Set seeds for reproducibility \"\"\"\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        \n",
    "\n",
    "set_seeds(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "09a6c203",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33moypio\u001b[0m (\u001b[33moypio-kpi\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/yaroslavp/workspace/master_IASA/iasa_nlp_course/mylabs/wandb/run-20250530_003806-t1z5vgig</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/oypio-kpi/iasa-nlp-labs/runs/t1z5vgig' target=\"_blank\">lab2-t5-base-fr-en</a></strong> to <a href='https://wandb.ai/oypio-kpi/iasa-nlp-labs' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/oypio-kpi/iasa-nlp-labs' target=\"_blank\">https://wandb.ai/oypio-kpi/iasa-nlp-labs</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/oypio-kpi/iasa-nlp-labs/runs/t1z5vgig' target=\"_blank\">https://wandb.ai/oypio-kpi/iasa-nlp-labs/runs/t1z5vgig</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/oypio-kpi/iasa-nlp-labs/runs/t1z5vgig?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7ce6df0b1350>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.init(\n",
    "    project=\"iasa-nlp-labs\",\n",
    "    entity=\"oypio-kpi\", \n",
    "    name='lab2-t5-base-fr-en'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570efed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yaroslavp/workspace/master_IASA/iasa_nlp_course/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "import evaluate\n",
    "from transformers import (\n",
    "    MT5ForConditionalGeneration,\n",
    "    MT5Tokenizer,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    DataCollatorForSeq2Seq,\n",
    ")\n",
    "\n",
    "model_name = \"google/mt5-base\"\n",
    "tokenizer = MT5Tokenizer.from_pretrained(model_name)\n",
    "model = MT5ForConditionalGeneration.from_pretrained(model_name)\n",
    "MAX_LEN = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e707ceb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33de34365f404d3bb4d129150ed5caeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yaroslavp/workspace/master_IASA/iasa_nlp_course/.venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:3921: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def preprocess_function(batch):\n",
    "    prefix = \"translate French to English: \"\n",
    "    inputs = [prefix + text for text in batch['fr']]\n",
    "    targets = batch['en']\n",
    "    # Tokenize without padding (dynamic padding will be applied in the collator)\n",
    "    model_inputs = tokenizer(inputs, max_length=MAX_LEN, truncation=True, padding=False)\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(targets, max_length=MAX_LEN, truncation=True, padding=False)\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "# Tokenize the dataset\n",
    "tokenized_dataset = data.map(preprocess_function, batched=True)\n",
    "\n",
    "# Split the dataset into training and evaluation sets\n",
    "split_datasets = tokenized_dataset.train_test_split(test_size=0.05)\n",
    "train_dataset = split_datasets[\"train\"]\n",
    "eval_dataset = split_datasets[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6c721d4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'en': 'Here it is.',\n",
       " 'fr': 'Le voilà.',\n",
       " 'input_ids': [37194, 21273, 288, 5413, 267, 764, 3133, 23068, 260, 1],\n",
       " 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       " 'labels': [10421, 609, 339, 260, 1]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f35b64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [[label.strip()] for label in labels]\n",
    "\n",
    "    return preds, labels\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    try:\n",
    "        preds, labels = eval_preds\n",
    "        if isinstance(preds, tuple):\n",
    "            preds = preds[0]\n",
    "\n",
    "        preds = np.where(preds != -100, preds, tokenizer.pad_token_id)\n",
    "        decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=False)\n",
    "    \n",
    "        # Replace -100 in the labels as we can't decode them.\n",
    "        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=False)\n",
    "    \n",
    "        # Some simple post-processing\n",
    "        decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "    \n",
    "        result_bleu = bleu_score.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "        result_bertscore = bert_score.compute(predictions=decoded_preds, references=decoded_labels, lang=\"en\")\n",
    "        prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
    "\n",
    "        result = {\n",
    "            \"bleu\": result_bleu[\"bleu\"], \n",
    "            \"bertscore_f1\": np.mean(result_bertscore[\"f1\"]),\n",
    "            \"bertscore_precision\": np.mean(result_bertscore[\"precision\"]),\n",
    "            \"bertscore_recall\": np.mean(result_bertscore[\"recall\"]),\n",
    "            \"gen_len\": np.mean(prediction_lens)\n",
    "        }\n",
    "    \n",
    "        result = {k: round(v, 4) for k, v in result.items()}\n",
    "        return result\n",
    "    except Exception as exc:\n",
    "        print(str(exc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bb099e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, label_pad_token_id=-100)\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./mt5_fr2en\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    predict_with_generate=True,  # This ensures that model.generate() is used for predictions. Think about where do we need it?\n",
    "    num_train_epochs=15,\n",
    "    learning_rate=1e-4,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=1,\n",
    "    push_to_hub=False\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "36c61681",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yaroslavp/workspace/master_IASA/iasa_nlp_course/.venv/lib/python3.11/site-packages/transformers/generation/utils.py:1141: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yaroslavp/workspace/master_IASA/iasa_nlp_course/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "inference_output = trainer.predict(eval_dataset)\n",
    "\n",
    "\n",
    "# Decode generated predictions\n",
    "generated_texts = tokenizer.batch_decode(\n",
    "    inference_output.predictions,\n",
    "    skip_special_tokens=True,\n",
    "    clean_up_tokenization_spaces=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f2a79ab8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test_loss': 12.52834415435791,\n",
       " 'test_bleu': 0.335,\n",
       " 'test_bertscore_f1': 0.8312,\n",
       " 'test_bertscore_precision': 0.8412,\n",
       " 'test_bertscore_recall': 0.8248,\n",
       " 'test_gen_len': 5.34,\n",
       " 'test_runtime': 3.0481,\n",
       " 'test_samples_per_second': 16.403,\n",
       " 'test_steps_per_second': 1.312}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inference_output.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6a280384",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<extra_id_0> / Archives',\n",
       " '<extra_id_0> - Peter',\n",
       " '<extra_id_0> -',\n",
       " '<extra_id_0>',\n",
       " '<extra_id_0> : -',\n",
       " '<extra_id_0>.',\n",
       " '<extra_id_0> - french.',\n",
       " '<extra_id_0>!',\n",
       " '<extra_id_0>.',\n",
       " '<extra_id_0> -',\n",
       " '<extra_id_0>?',\n",
       " '<extra_id_0> french to English',\n",
       " '<extra_id_0> french to english',\n",
       " '<extra_id_0>?',\n",
       " '<extra_id_0> asthm',\n",
       " '<extra_id_0>',\n",
       " '<extra_id_0> - french',\n",
       " '<extra_id_0>.',\n",
       " '<extra_id_0> french dictionary',\n",
       " '<extra_id_0> French (CA)',\n",
       " '<extra_id_0> -?',\n",
       " '<extra_id_0> non non non',\n",
       " '<extra_id_0> - eHow',\n",
       " '<extra_id_0> -?',\n",
       " '<extra_id_0> à droit',\n",
       " '<extra_id_0> - Wikimedia Foundation',\n",
       " '<extra_id_0> - french to English:',\n",
       " '<extra_id_0> -  <extra_id_1> -',\n",
       " '<extra_id_0> English - French',\n",
       " '<extra_id_0>?',\n",
       " '<extra_id_0> - french.',\n",
       " '<extra_id_0>.',\n",
       " '<extra_id_0>:?',\n",
       " '<extra_id_0> - french.',\n",
       " '<extra_id_0>.',\n",
       " '<extra_id_0> great great great great great',\n",
       " '<extra_id_0>.',\n",
       " '<extra_id_0> :',\n",
       " '<extra_id_0> French to English',\n",
       " '<extra_id_0> french to English:',\n",
       " '<extra_id_0> - french.',\n",
       " '<extra_id_0> - french.',\n",
       " '<extra_id_0>.',\n",
       " '<extra_id_0> french to english',\n",
       " '<extra_id_0> Cette montre',\n",
       " '<extra_id_0> french to English:',\n",
       " '<extra_id_0> French to Spanish',\n",
       " '<extra_id_0> french to English',\n",
       " '<extra_id_0> french to english',\n",
       " '<extra_id_0> translate - French.']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0ee3048d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='900' max='900' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [900/900 18:35, Epoch 15/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Bertscore F1</th>\n",
       "      <th>Bertscore Precision</th>\n",
       "      <th>Bertscore Recall</th>\n",
       "      <th>Gen Len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>11.744500</td>\n",
       "      <td>6.100876</td>\n",
       "      <td>0.316800</td>\n",
       "      <td>0.828900</td>\n",
       "      <td>0.833800</td>\n",
       "      <td>0.826900</td>\n",
       "      <td>5.780000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>7.001600</td>\n",
       "      <td>3.743660</td>\n",
       "      <td>0.366200</td>\n",
       "      <td>0.841700</td>\n",
       "      <td>0.884800</td>\n",
       "      <td>0.805000</td>\n",
       "      <td>4.360000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4.178900</td>\n",
       "      <td>2.621285</td>\n",
       "      <td>0.160400</td>\n",
       "      <td>0.852000</td>\n",
       "      <td>0.883800</td>\n",
       "      <td>0.824200</td>\n",
       "      <td>9.740000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3.316900</td>\n",
       "      <td>2.369670</td>\n",
       "      <td>0.164200</td>\n",
       "      <td>0.863200</td>\n",
       "      <td>0.877200</td>\n",
       "      <td>0.851100</td>\n",
       "      <td>9.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.825700</td>\n",
       "      <td>2.255475</td>\n",
       "      <td>0.230900</td>\n",
       "      <td>0.877700</td>\n",
       "      <td>0.886800</td>\n",
       "      <td>0.869700</td>\n",
       "      <td>7.420000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2.489600</td>\n",
       "      <td>2.239475</td>\n",
       "      <td>0.236700</td>\n",
       "      <td>0.875500</td>\n",
       "      <td>0.890400</td>\n",
       "      <td>0.862200</td>\n",
       "      <td>7.260000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>2.343400</td>\n",
       "      <td>2.174262</td>\n",
       "      <td>0.213400</td>\n",
       "      <td>0.873600</td>\n",
       "      <td>0.888900</td>\n",
       "      <td>0.860000</td>\n",
       "      <td>7.940000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>2.192300</td>\n",
       "      <td>2.149492</td>\n",
       "      <td>0.221000</td>\n",
       "      <td>0.876300</td>\n",
       "      <td>0.888500</td>\n",
       "      <td>0.865300</td>\n",
       "      <td>7.740000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>2.064200</td>\n",
       "      <td>2.178177</td>\n",
       "      <td>0.206600</td>\n",
       "      <td>0.872300</td>\n",
       "      <td>0.887400</td>\n",
       "      <td>0.858700</td>\n",
       "      <td>8.140000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.939200</td>\n",
       "      <td>2.158738</td>\n",
       "      <td>0.210500</td>\n",
       "      <td>0.873800</td>\n",
       "      <td>0.891600</td>\n",
       "      <td>0.857900</td>\n",
       "      <td>8.060000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.842200</td>\n",
       "      <td>2.151050</td>\n",
       "      <td>0.209400</td>\n",
       "      <td>0.873400</td>\n",
       "      <td>0.889200</td>\n",
       "      <td>0.859300</td>\n",
       "      <td>8.060000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.784700</td>\n",
       "      <td>2.141221</td>\n",
       "      <td>0.205700</td>\n",
       "      <td>0.871400</td>\n",
       "      <td>0.887700</td>\n",
       "      <td>0.856800</td>\n",
       "      <td>8.180000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>1.782700</td>\n",
       "      <td>2.147461</td>\n",
       "      <td>0.214000</td>\n",
       "      <td>0.871600</td>\n",
       "      <td>0.889100</td>\n",
       "      <td>0.855900</td>\n",
       "      <td>7.940000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1.724200</td>\n",
       "      <td>2.141468</td>\n",
       "      <td>0.220200</td>\n",
       "      <td>0.872600</td>\n",
       "      <td>0.888200</td>\n",
       "      <td>0.858500</td>\n",
       "      <td>7.740000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.688400</td>\n",
       "      <td>2.141897</td>\n",
       "      <td>0.222700</td>\n",
       "      <td>0.872100</td>\n",
       "      <td>0.887500</td>\n",
       "      <td>0.858300</td>\n",
       "      <td>7.660000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yaroslavp/workspace/master_IASA/iasa_nlp_course/.venv/lib/python3.11/site-packages/transformers/generation/utils.py:1141: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=900, training_loss=3.2612314860026044, metrics={'train_runtime': 1116.0188, 'train_samples_per_second': 12.769, 'train_steps_per_second': 0.806, 'total_flos': 1962112532649984.0, 'train_loss': 3.2612314860026044, 'epoch': 15.0})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "48bde33b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['en', 'fr', 'input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 50\n",
       "})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "81de2bd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.to(\"cuda\")\n",
    "\n",
    "inference_output = trainer.predict(eval_dataset)\n",
    "\n",
    "\n",
    "# Decode generated predictions\n",
    "generated_texts = tokenizer.batch_decode(\n",
    "    inference_output.predictions,\n",
    "    skip_special_tokens=True,\n",
    "    clean_up_tokenization_spaces=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "544449c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test_loss': 2.141897201538086,\n",
       " 'test_bleu': 0.2227,\n",
       " 'test_bertscore_f1': 0.8721,\n",
       " 'test_bertscore_precision': 0.8875,\n",
       " 'test_bertscore_recall': 0.8583,\n",
       " 'test_gen_len': 7.66,\n",
       " 'test_runtime': 3.1738,\n",
       " 'test_samples_per_second': 15.754,\n",
       " 'test_steps_per_second': 1.26}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inference_output.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "582b6135",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline(\n",
    "    \"text2text-generation\", model=trainer.model.eval(), tokenizer=trainer.tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "29eab917",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Hello, how are you gonna go?'}]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = generator(\n",
    "    \"translate French to English: \" + \"Bonjour comment allez-vous?\",\n",
    "    max_length=128,\n",
    "    do_sample=True,\n",
    "    temperature=0.8\n",
    ")\n",
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314978a8",
   "metadata": {},
   "source": [
    "Завантаження збереженої моделі"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cdb022e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yaroslavp/workspace/master_IASA/iasa_nlp_course/.venv/lib/python3.11/site-packages/transformers/generation/utils.py:1141: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yaroslavp/workspace/master_IASA/iasa_nlp_course/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import pipeline\n",
    "import evaluate\n",
    "from transformers import (\n",
    "    MT5ForConditionalGeneration,\n",
    "    MT5Tokenizer,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    DataCollatorForSeq2Seq,\n",
    ")\n",
    "\n",
    "model_name = \"./mt5_fr2en_copy/checkpoint-500\"\n",
    "tokenizer = MT5Tokenizer.from_pretrained(model_name)\n",
    "model = MT5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, label_pad_token_id=-100)\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./mt5_fr2en\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    predict_with_generate=True,  # This ensures that model.generate() is used for predictions. Think about where do we need it?\n",
    "    num_train_epochs=15,\n",
    "    learning_rate=1e-4,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=1,\n",
    "    push_to_hub=False\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "model.to(\"cuda\")\n",
    "\n",
    "inference_output = trainer.predict(eval_dataset)\n",
    "\n",
    "generated_texts = tokenizer.batch_decode(\n",
    "    inference_output.predictions,\n",
    "    skip_special_tokens=True,\n",
    "    clean_up_tokenization_spaces=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8a869281",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test_loss': 1.1302889585494995,\n",
       " 'test_bleu': 0.0114,\n",
       " 'test_bertscore_f1': 0.8398,\n",
       " 'test_bertscore_precision': 0.8674,\n",
       " 'test_bertscore_recall': 0.8162,\n",
       " 'test_gen_len': 8.58,\n",
       " 'test_runtime': 3.6516,\n",
       " 'test_samples_per_second': 13.692,\n",
       " 'test_steps_per_second': 1.095}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inference_output.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "20c0ac80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Hello, how are you doing now?'}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator = pipeline(\n",
    "    \"text2text-generation\", model=trainer.model.eval(), tokenizer=trainer.tokenizer\n",
    ")\n",
    "\n",
    "outputs = generator(\n",
    "    \"translate French to English: \" + \"Bonjour comment allez-vous?\",\n",
    "    max_length=128,\n",
    "    do_sample=True,\n",
    "    temperature=0.8\n",
    ")\n",
    "outputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0815b4a8",
   "metadata": {},
   "source": [
    "### Файнтюн decoder-only LLM моделі"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56d9126d",
   "metadata": {},
   "outputs": [],
   "source": [
    "PRETRAINED_MODEL = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "MAX_LEN = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44eb83cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yaroslavp/workspace/master_IASA/iasa_nlp_course/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n",
      "/home/yaroslavp/workspace/master_IASA/iasa_nlp_course/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "nf4_config = BitsAndBytesConfig(\n",
    "   load_in_4bit=True,\n",
    "   bnb_4bit_quant_type=\"nf4\",\n",
    "   bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(PRETRAINED_MODEL, use_fast=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    PRETRAINED_MODEL,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    quantization_config=nf4_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "352f6319",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 2048)\n",
       "    (layers): ModuleList(\n",
       "      (0-21): 22 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=2048, out_features=256, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=2048, out_features=256, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear4bit(in_features=2048, out_features=5632, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=2048, out_features=5632, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=5632, out_features=2048, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3a4a7ab5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b432680639c549c7ab9d1f144f243831",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yaroslavp/workspace/master_IASA/iasa_nlp_course/.venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:3921: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def preprocess_function(batch):\n",
    "    prefix = \"translate French to English: \"\n",
    "    inputs = [prefix + text for text in batch['fr']]\n",
    "    targets = batch['en']\n",
    "    # Tokenize without padding (dynamic padding will be applied in the collator)\n",
    "    model_inputs = tokenizer(inputs, max_length=MAX_LEN, truncation=True, padding=False)\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(targets, max_length=MAX_LEN, truncation=True, padding=False)\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "# Tokenize the dataset\n",
    "tokenized_dataset = data.map(preprocess_function, batched=True)\n",
    "\n",
    "# Split the dataset into training and evaluation sets\n",
    "split_datasets = tokenized_dataset.train_test_split(test_size=0.05)\n",
    "train_dataset = split_datasets[\"train\"]\n",
    "eval_dataset = split_datasets[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "84e732c2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'clear_device_cache' from 'accelerate.utils.memory' (/home/yaroslavp/workspace/master_IASA/iasa_nlp_course/.venv/lib/python3.11/site-packages/accelerate/utils/memory.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BitsAndBytesConfig\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpeft\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_peft_config, prepare_model_for_kbit_training, PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType\n\u001b[32m      5\u001b[39m lora_config = LoraConfig(\n\u001b[32m      6\u001b[39m     r=\u001b[32m8\u001b[39m,  \u001b[38;5;66;03m# the dimension of the low-rank matrices\u001b[39;00m\n\u001b[32m      7\u001b[39m     lora_alpha=\u001b[32m16\u001b[39m, \u001b[38;5;66;03m# scaling factor for LoRA activations vs pre-trained weight activations\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     12\u001b[39m     target_modules=[\u001b[33m'\u001b[39m\u001b[33mo_proj\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mv_proj\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mq_proj\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mk_proj\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mgate_proj\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mdown_proj\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mup_proj\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     13\u001b[39m ) \n\u001b[32m     15\u001b[39m model = prepare_model_for_kbit_training(model)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/master_IASA/iasa_nlp_course/.venv/lib/python3.11/site-packages/peft/__init__.py:17\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Copyright 2023-present the HuggingFace Inc. team.\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[32m     15\u001b[39m __version__ = \u001b[33m\"\u001b[39m\u001b[33m0.15.2\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mauto\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     18\u001b[39m     MODEL_TYPE_TO_PEFT_MODEL_MAPPING,\n\u001b[32m     19\u001b[39m     AutoPeftModel,\n\u001b[32m     20\u001b[39m     AutoPeftModelForCausalLM,\n\u001b[32m     21\u001b[39m     AutoPeftModelForFeatureExtraction,\n\u001b[32m     22\u001b[39m     AutoPeftModelForQuestionAnswering,\n\u001b[32m     23\u001b[39m     AutoPeftModelForSeq2SeqLM,\n\u001b[32m     24\u001b[39m     AutoPeftModelForSequenceClassification,\n\u001b[32m     25\u001b[39m     AutoPeftModelForTokenClassification,\n\u001b[32m     26\u001b[39m )\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfig\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PeftConfig, PromptLearningConfig\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmapping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     29\u001b[39m     PEFT_TYPE_TO_CONFIG_MAPPING,\n\u001b[32m     30\u001b[39m     PEFT_TYPE_TO_MIXED_MODEL_MAPPING,\n\u001b[32m   (...)\u001b[39m\u001b[32m     33\u001b[39m     inject_adapter_in_model,\n\u001b[32m     34\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/master_IASA/iasa_nlp_course/.venv/lib/python3.11/site-packages/peft/auto.py:31\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Optional\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     22\u001b[39m     AutoModel,\n\u001b[32m     23\u001b[39m     AutoModelForCausalLM,\n\u001b[32m   (...)\u001b[39m\u001b[32m     28\u001b[39m     AutoTokenizer,\n\u001b[32m     29\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfig\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PeftConfig\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpeft_model\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     33\u001b[39m     PeftModel,\n\u001b[32m     34\u001b[39m     PeftModelForCausalLM,\n\u001b[32m   (...)\u001b[39m\u001b[32m     39\u001b[39m     PeftModelForTokenClassification,\n\u001b[32m     40\u001b[39m )\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconstants\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TOKENIZER_CONFIG_NAME\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/master_IASA/iasa_nlp_course/.venv/lib/python3.11/site-packages/peft/config.py:24\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mhuggingface_hub\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m hf_hub_download\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PushToHubMixin\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CONFIG_NAME, PeftType, TaskType\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# we expect at least these keys to be present in a PEFT adapter_config.json\u001b[39;00m\n\u001b[32m     28\u001b[39m MIN_EXPECTED_CONFIG_KEYS = {\u001b[33m\"\u001b[39m\u001b[33mpeft_type\u001b[39m\u001b[33m\"\u001b[39m}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/master_IASA/iasa_nlp_course/.venv/lib/python3.11/site-packages/peft/utils/__init__.py:16\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Copyright 2023-present the HuggingFace Inc. team.\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mintegrations\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m map_cache_to_layer_device_map\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mloftq_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m replace_lora_weights_loftq\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mother\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     18\u001b[39m     CONFIG_NAME,\n\u001b[32m     19\u001b[39m     INCLUDE_LINEAR_LAYERS_SHORTHAND,\n\u001b[32m   (...)\u001b[39m\u001b[32m     50\u001b[39m     transpose,\n\u001b[32m     51\u001b[39m )\n\u001b[32m     52\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpeft_types\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PeftType, TaskType, register_peft_method\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/master_IASA/iasa_nlp_course/.venv/lib/python3.11/site-packages/peft/utils/loftq_utils.py:25\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Callable, Optional, Union\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01maccelerate\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmemory\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m clear_device_cache\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mhuggingface_hub\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m snapshot_download\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mhuggingface_hub\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01merrors\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m HFValidationError, LocalEntryNotFoundError\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'clear_device_cache' from 'accelerate.utils.memory' (/home/yaroslavp/workspace/master_IASA/iasa_nlp_course/.venv/lib/python3.11/site-packages/accelerate/utils/memory.py)"
     ]
    }
   ],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "from peft import get_peft_config, prepare_model_for_kbit_training, PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,  # the dimension of the low-rank matrices\n",
    "    lora_alpha=16, # scaling factor for LoRA activations vs pre-trained weight activations\n",
    "    lora_dropout=0.05, \n",
    "    bias='none',\n",
    "    inference_mode=False,\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    target_modules=['o_proj', 'v_proj', \"q_proj\", \"k_proj\", \"gate_proj\", \"down_proj\", \"up_proj\"]\n",
    ") \n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, lora_config)\n",
    "# Trainable Parameters\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2dd53ef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accelerate version: 0.23.0\n"
     ]
    }
   ],
   "source": [
    "import accelerate\n",
    "# import peft\n",
    "\n",
    "print(\"Accelerate version:\", accelerate.__version__)\n",
    "# print(\"PEFT version:\", peft.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7fb9b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "# Initialize with team/entity\n",
    "wandb.init(\n",
    "    project=\"iasa-nlp-labs\",\n",
    "    entity=\"oypio-kpi\", \n",
    "    name='llama3-1b-pretrain'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2b7087",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import XLMRobertaConfig, XLMRobertaTokenizer, XLMRobertaForMaskedLM\n",
    "from transformers import LineByLineTextDataset, DataCollatorForLanguageModeling\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "\n",
    "# Data collator used for dynamic masking\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=False\n",
    ")\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./model_checkpoints_llama3_pretrain',\n",
    "    logging_dir='./model_logs_llama3_pretrain',\n",
    "    weight_decay=0.01,\n",
    "    learning_rate=1e-4,\n",
    "    lr_scheduler_type='cosine',\n",
    "    warmup_ratio=0.0,\n",
    "    num_train_epochs=2,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    gradient_accumulation_steps=2,\n",
    "    do_train=True,\n",
    "    do_eval=False,\n",
    "    bf16=True,\n",
    "    report_to=\"wandb\",\n",
    "    optim='adamw_8bit',\n",
    "    save_strategy=\"steps\",\n",
    "    logging_steps=5\n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_dataset['train']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4e5fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff726a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text2text-generationn\",\n",
    "    model=trainer.model.eval(),\n",
    "    tokenizer=tokenizer,\n",
    "    model_kwargs={\n",
    "        \"torch_dtype\": torch.bfloat16, #use float16 for non A100/H100\n",
    "        \"quantization_config\": {\"load_in_4bit\": True},\n",
    "        \"device_map\": \"cuda:0\"\n",
    "    }\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
